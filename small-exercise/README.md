In this file, there are some exercises of data analysis methods and datamining algorithms.
Here are the definitions of each method and algorithm as following.
- <strong>Factor Analysis (FA)</strong>

  <em>Factor analysis (FA)</em> is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in say six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors, plus "error" terms. The information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis originated in psychometrics and is used in behavioral sciences, social sciences, marketing, product management, operations research, and other fields that deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables.

> https://en.wikipedia.org/wiki/Factor_analysis#cite_note-Bartholomew2008-1

- <strong>Principal Components Analysis (PCA)</strong>

 <em>Principal component analysis (PCA)</em> is a statistical procedure that uses an orthogonal transformation to convert a set of 
observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal 
components. The number of principal components is less than or equal to the number of original variables. This transformation 
is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much 
of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the 
constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. 
The principal components are orthogonal because they are the eigenvectors of the covariance matrix, which is symmetric. PCA is 
sensitive to the relative scaling of the original variables.

> https://en.wikipedia.org/wiki/Principal_component_analysis
