\documentclass[12pt,a4paper]{book}

<<libraries,eval=TRUE,include=FALSE>>=
library(MASS)
library(RColorBrewer)
library(gplots)
library(snowfall)
library(ggplot2)
library(xtable)
@

\usepackage{geometry}
\geometry{left=2cm,right=2cm,top=2cm,bottom=2cm}
\renewcommand{\baselinestretch}{1.25}

\usepackage{graphicx}

%The bm package defines a command \bm which makes its argument bold. The argument may be any maths object from a single symbol to an expression.
\usepackage{bm}

% sous titres
\usepackage{subfig}

% package to generate commands with double letters in maths
\usepackage{dsfont}
\usepackage{amsfonts}
\newcommand{\uns}[1]{\mathds{1}[ #1 ]}
\newcommand{\esp}[1]{\mathbb{E}[ #1 ]}
\newcommand{\var}[1]{\mathbb{V}[ #1 ]}
\newcommand\Ind{\protect\mathpalette{\protect\independenT}{\perp}}


\begin{document}


\title{EPE Data Analysis}
\author{\textbf{Jingwen ZHENG}\\ Toulouse School of Economics\\ M2 ERNA}
\maketitle

\tableofcontents

  \chapter{The Two Fundamental Problems of Inference}
    \section{The Fundamental Problem of Causal Inference}  

      \subsection{generate data with selection rule $D_i = \uns{y_i^B\leq\bar{y}}$}

<<param,eval=TRUE,echo=FALSE,results='hide'>>=
param <- c(7,.77,.1,1700,0.5,0.1,0.05,0.07,0.05,0.01)
names(param) <- c("barmu","sigma2mu","sigma2U","barY","rho","theta","sigma2epsilon","sigma2eta","delta","baralpha")
@

<<delta.y.tt,eval=TRUE,echo=FALSE,results='hide'>>=
delta.y.tt <- function(param){
  return(param["baralpha"]+param["theta"]*param["barmu"]-param["theta"]*((param["sigma2mu"]*dnorm((log(param["barY"])-param["barmu"])/(sqrt(param["sigma2mu"]+param["sigma2U"]))))/(sqrt(param["sigma2mu"]+param["sigma2U"])*pnorm((log(param["barY"])-param["barmu"])/(sqrt(param["sigma2mu"]+param["sigma2U"]))))))
}
@

<<data.FPCI,eval=TRUE,echo=FALSE,results='hide'>>=
set.seed(6789)
NC <-2000
mu <- rnorm(NC,param["barmu"],sqrt(param["sigma2mu"]))
UB <- rnorm(NC,0,sqrt(param["sigma2U"]))
yB <- mu + UB 
Ds <- rep(0,NC)
Ds[yB<=log(param["barY"])] <- 1 
epsilon <- rnorm(NC,0,sqrt(param["sigma2epsilon"]))
eta<- rnorm(NC,0,sqrt(param["sigma2eta"]))
U0 <- param["rho"]*UB + epsilon
y0 <- mu +  U0 + param["delta"]
alpha <- param["baralpha"]+  param["theta"]*mu + eta
y1 <- y0+alpha
Y0 <- exp(y0)
Y1 <- exp(y1)
y <- y1*Ds+y0*(1-Ds)
Y <- Y1*Ds+Y0*(1-Ds)
@

      \subsection{plot potential outcomes and observed outcomes}
<<Pot.obs.out.FPCI,eval=TRUE,fig.cap='Potential outcomes and observed outcomes',fig.subcap=c('Potential outcomes','Observed outcomes'),fig.align='center',out.width='.5\\textwidth',echo=FALSE,results='hide',fig.pos='htbp'>>=
col.obs <- 'black'
col.unobs <- 'red'

plot(yB[Ds==0],y0[Ds==0],pch=1,xlim=c(4,11),ylim=c(4,11),xlab="yB",ylab="Outcomes")
points(yB[Ds==1],y1[Ds==1],pch=3)
points(yB[Ds==0],y1[Ds==0],pch=3,col=col.unobs)
points(yB[Ds==1],y0[Ds==1],pch=1,col=col.unobs)
abline(v=log(param["barY"]),col=col.unobs)
legend(4,11,c('y0|D=0','y1|D=1','y0|D=1','y1|D=0'),pch=c(1,3,1,3),col=c(col.obs,col.obs,col.unobs,col.unobs),ncol=2)

plot(yB[Ds==0],y0[Ds==0],pch=1,xlim=c(4,11),ylim=c(4,11),xlab="yB",ylab="Outcomes")
points(yB[Ds==1],y1[Ds==1],pch=3)
legend(4,11,c('y|D=0','y|D=1'),pch=c(1,3))
abline(v=log(param["barY"]),col=col.unobs)
@
      
      \subsection{Compute individual level treatment effects in the sample}
      
In order to compute individual level treatment effects in the sample, I use the formula: \\ 
\centerline{$\Delta_i^Y =Y_i^1 -Y^0_i$} 

<<ind.tt.NC,eval=TRUE,echo=FALSE>>=
it<- y1-y0
@

\noindent{Here, I get the individual level treatment effects is \Sexpr{it[1:NC]}.}

      \subsection{Compute the average treatment effect on the treated in the sample by taking the average of the individual level treatment effects of the treated}
  
<<avg.tt,eval=TRUE,echo=FALSE>>=
avg.tt<-mean(it)
@
By computing the average of the individual level treatment effects of the treated, I get the average treatment effect on the treated in the sample is \textbf{\Sexpr{avg.tt}}.

      \subsection{Compare its value with the theoretical one in the population}
According to the course, we learnt the theoretical average treatment effect with the following formula:\\
\centerline{$\Large\Delta_{TT}^y =\bar{\alpha}+\theta\bar{\mu} -\theta\displaystyle\frac{\sigma^2_{\mu}}{\sqrt{\sigma^2_{\mu}+\sigma^2_{U}}}\frac{\displaystyle\phi\left(\frac{\bar{y}-\bar{\mu}}{\sqrt{\sigma^2_{\mu}+\sigma^2_{U}}}\right)}{\displaystyle\Phi\left(\frac{\bar{y}-\bar{\mu}}{\sqrt{\sigma^2_{\mu}+\sigma^2_{U}}}\right)}$}\\

<<theo.avg.tt,eval=TRUE,echo=FALSE>>=
theo.avg.tt<-delta.y.tt(param)
@

\noindent{Thus, I get the theoretical average treatment effect is \textbf{\Sexpr{theo.avg.tt}}, which is smaller than the value of last question.}

      \subsection{Compute the WW estimator in the sample}
In order to compute the WW estimator in the sample, I use the formula:\\
\centerline{$\Delta^Y_{WW} = \esp{Y_i|D_i=1} - \esp{Y_i|D_i=0}$}

<<WW.esti,eval=TRUE,echo=FALSE>>=
WW.esti<-mean(y[Ds==1])-mean(y[Ds==0])
@

\noindent{Here, I get the WW estimator in the sample is $\bm{\Sexpr{WW.esti}}$.}

      \subsection{Compute Selection Bias}
According to the course, we get that $\Delta^Y_{WW} = \Delta^Y_{TT} + \Delta^Y_{SB}$, so $\Delta^Y_{SB} = \Delta^Y_{WW} - \Delta^Y_{TT}$ . 

<<SB,eval=TRUE,echo=FALSE>>=
SB<-abs(WW.esti-avg.tt)
@

\noindent{With the $\Delta^Y_{TT}$ and $\Delta^Y_{WW}$ that I got before, I can get the Selecton Bias is $\bm{\Sexpr{SB}}$.}

      \subsection{Compare its value to the theoretical one in the sample}
According to the course, we learnt the theoretical selection bias with the following formula:\\ \\
\centerline{$\Large\Delta_{SB}^y = -\displaystyle\frac{\sigma^2_{\mu}+\rho\sigma^2_{U}}{\sqrt{\sigma^2_{\mu}+\sigma^2_{U}}}\left(\frac{\displaystyle\phi\left(\frac{\bar{y}-\bar{\mu}}{\sqrt{\sigma^2_{\mu}+\sigma^2_{U}}}\right)}{\displaystyle\Phi\left(\frac{\bar{y}-\bar{\mu}}{\sqrt{\sigma^2_{\mu}+\sigma^2_{U}}}\right)}+\frac{\displaystyle\phi\left(\frac{\bar{y}-\bar{\mu}}{\sqrt{\sigma^2_{\mu}+\sigma^2_{U}}}\right)}{1-\displaystyle\Phi\left(\frac{\bar{y}-\bar{\mu}}{\sqrt{\sigma^2_{\mu}+\sigma^2_{U}}}\right)}\right)$} \\

<<WW.SB,eval=TRUE,echo=FALSE,results='hide'>>=
delta.y.sb <- function(param){
  return(-(param["sigma2mu"]+param["rho"]*param["sigma2U"])/sqrt(param["sigma2mu"]+param["sigma2U"])*dnorm((log(param["barY"])-param["barmu"])/(sqrt(param["sigma2mu"]+param["sigma2U"])))*(1/pnorm((log(param["barY"])-param["barmu"])/(sqrt(param["sigma2mu"]+param["sigma2U"])))+1/(1-pnorm((log(param["barY"])-param["barmu"])/(sqrt(param["sigma2mu"]+param["sigma2U"]))))))
}
@

<<theo.SB,eval=TRUE,echo=FALSE>>=
theo.SB<-abs(delta.y.sb(param))
@

\noindent{Thus, I get the theoretical average treatment effect is $\bm{\Sexpr{theo.SB}}$, which is smaller than the value of last question.}

      \subsection{Compute the BA estimator in the sample}
In order to compute the BA estimator in the sample, I use the formula:\\
\centerline{$\Delta^Y_{BA} = \esp{Y_i|D_i=1} - \esp{Y_i^B|D_i=1}$}

<<BA.esti,eval=TRUE,echo=FALSE>>=
BA.esti<-mean(y[Ds==1])-mean(yB[Ds==1])
@

\noindent{Here, I get the BA estimator in the sample is $\bm{\Sexpr{BA.esti}}$.}

      \subsection{Compute its bias}
According to the course, we get that $\Delta^Y_{BA} = \Delta^Y_{TT} + \Delta^Y_{TB}$, so $\Delta^Y_{TB} = \Delta^Y_{BA} - \Delta^Y_{TT}$ . 

<<TB,eval=TRUE,echo=FALSE>>=
TB<-BA.esti-avg.tt
@

\noindent{With the $\Delta^Y_{TT}$ and $\Delta^Y_{BA}$ that I got before, I can get the Time Trend Bias is $\bm{\Sexpr{TB}}$.}


  \section{The Fundamental Problem of Statistical Inference}
    
    \subsection{Generate data with selection rule $D_i = \uns{V_i\leq\bar{y}}$}
<<param.FPSI,eval=TRUE,echo=FALSE,results='hide'>>=
paramS <- c(5,.45,.3,600,0.8,0.06,0.05,0.2,0.05,0.1)
names(paramS) <- c("barmu","sigma2mu","sigma2U","barY","rho","theta","sigma2epsilon","sigma2eta","delta","baralpha")
@

<<data.FPSI,eval=TRUE,echo=FALSE,results='hide'>>=
data.FPSI<-set.seed(8778)
NS <-1500
mu <- rnorm(NS,paramS["barmu"],sqrt(paramS["sigma2mu"]))
UB <- rnorm(NS,0,sqrt(paramS["sigma2U"]))
V <- mu + UB 
Ds <- rep(0,NS)
Ds[V<=log(paramS["barY"])] <- 1 
epsilon <- rnorm(NS,0,sqrt(paramS["sigma2epsilon"]))
eta<- rnorm(NS,0,sqrt(paramS["sigma2eta"]))
U0 <- paramS["rho"]*UB + epsilon
y0 <- mu +  U0 + paramS["delta"]
alpha <- paramS["baralpha"]+  paramS["theta"]*mu + eta
y1 <- y0+alpha
Y0 <- exp(y0)
Y1 <- exp(y1)
y <- y1*Ds+y0*(1-Ds)
Y <- Y1*Ds+Y0*(1-Ds)
@

      \subsection{Plot potential outcomes and observed outcomes}
<<Pot.obs.out.FPSI,eval=TRUE,fig.cap='Potential outcomes and observed outcomes',fig.subcap=c('Potential outcomes','Observed outcomes'),fig.align='center',out.width='.5\\textwidth',echo=FALSE,results='hide',fig.pos='htbp'>>=
col.obs <- 'black'
col.unobs <- 'red'

plot(V[Ds==0],y0[Ds==0],pch=1,xlim=c(2,10),ylim=c(2,10),xlab="V",ylab="Outcomes")
points(V[Ds==1],y1[Ds==1],pch=3)
points(V[Ds==0],y1[Ds==0],pch=3,col=col.unobs)
points(V[Ds==1],y0[Ds==1],pch=1,col=col.unobs)
abline(v=log(paramS["barY"]),col=col.unobs)
legend(2,10,c('y0|D=0','y1|D=1','y0|D=1','y1|D=0'),pch=c(1,3,1,3),col=c(col.obs,col.obs,col.unobs,col.unobs),ncol=2)

plot(V[Ds==0],y0[Ds==0],pch=1,xlim=c(2,10),ylim=c(2,10),xlab="V",ylab="Outcomes")
points(V[Ds==1],y1[Ds==1],pch=3)
legend(2,10,c('y|D=0','y|D=1'),pch=c(1,3))
abline(v=log(paramS["barY"]),col=col.unobs)
@

      \subsection{Compute Individual level treatment effects in the sample}
In order to compute individual level treatment effects in the sample, I use the formula: \\ 
\centerline{$\Delta_i^Y =Y_i^1 -Y^0_i$} 

<<ind.tt.NS,eval=TRUE,echo=FALSE>>=
its<- y1-y0
@

\noindent{Here, I get the individual level treatment effects are \\ \Sexpr{its[1:NS]}}

      \subsection{Compute TT in the sample}
<<avg.tt.NS,eval=TRUE,echo=FALSE>>=
avg.tts<-mean(it)
@
By computing the average of the individual level treatment effects of the treated, I get the value of TT is \textbf{\Sexpr{avg.tts}}.

      \subsection{Compare with the theoretical value in the population}
According to the course, we learnt the theoretical average treatment effect with the following formula:\\
\centerline{$\Large\Delta_{TT}^y =\bar{\alpha}+\theta\bar{\mu} -\theta\displaystyle\frac{\sigma^2_{\mu}}{\sqrt{\sigma^2_{\mu}+\sigma^2_{U}}}\frac{\displaystyle\phi\left(\frac{\bar{y}-\bar{\mu}}{\sqrt{\sigma^2_{\mu}+\sigma^2_{U}}}\right)}{\displaystyle\Phi\left(\frac{\bar{y}-\bar{\mu}}{\sqrt{\sigma^2_{\mu}+\sigma^2_{U}}}\right)}$}\\

<<theo.avg.tts,eval=TRUE,echo=FALSE>>=
theo.avg.tts<-delta.y.tt(paramS)
@

\noindent{Thus, I get the theoretical average treatment effect is \textbf{\Sexpr{theo.avg.tts}}, which is smaller than the value of last question.}

      \subsection{Compute the WW estimator}
In order to compute the WW estimator in the sample, I use the formula:\\
\centerline{$\Delta^Y_{WW} = \esp{Y_i|D_i=1} - \esp{Y_i|D_i=0}$}

<<WW.estiS,eval=TRUE,echo=FALSE>>=
WW.estiS<-mean(y[Ds==1])-mean(y[Ds==0])
@

\noindent{Here, I get the WW estimator in the sample is $\bm{\Sexpr{WW.estiS}}$.}

      \subsection{Compute the OLS of beta in $y_i = \alpha + \beta D_i + U_i$}
According to the course, we learn the formula as following:\\ \\
\centerline{$\Large\hat{\beta}_{OLS} = \displaystyle\frac{\frac{1}{N}\sum_{i=1}^N\left(Y_i-\frac{1}{N}\sum_{i=1}^NY_i\right)\left(D_i-\frac{1}{N}\sum_{i=1}^ND_i\right)}{\frac{1}{N}\sum_{i=1}^N\left(D_i-\frac{1}{N}\sum_{i=1}^ND_i\right)^2}$}

<<btols,eval=TRUE,echo=FALSE>>=
a<-y-sum(y)/NS
b<-Ds-sum(Ds)/NS
betaOLS<-(sum(a*b)/NS)/(sum(b^2)/NS)
@

\noindent{Thus, I get the value of $\hat{\beta}_{OLS}$ is $\bm{\Sexpr{betaOLS}}$, which is the same as WW.}

      \subsection{Use the CLT formula to estimate the effect of sampling noise on WW with 99\% confidence. For this, estimate the variances of the outcomes of the treated and of the untreated in the sample. Do you find a result similar to mine?}

According to the course, we learnt the CLT formula to estimate the effect of sampling noise on WW: \\
\centerline{$\displaystyle\tilde{\epsilon} = \Phi^{-1}\left(\displaystyle\frac{\delta+1}{2}\right)\displaystyle\frac{1}{\sqrt{N}}\sqrt{\displaystyle\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\displaystyle\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}}$} \\

\noindent{In order to compute the value of $\tilde{\epsilon}$, I firstly compute $\var{Y_i^1|D_i=1}$ and $\var{Y_i^0|D_i=0}$.}

<<vars,eval=TRUE,echo=FALSE>>=
var1<-var(y[Ds==1])
var0<-var(y[Ds==0])
@

\noindent{According to R, I get $\var{Y_i^1|D_i=1}=\Sexpr{var1}$ and $\var{Y_i^0|D_i=0}=\Sexpr{var0}$.}

<<probas,eval=TRUE,echo=FALSE>>=
proba1<-length(Ds[Ds==1])/NS
@

\noindent{Moreover, I get $\Pr(D_i=1)=\Sexpr{proba1}$.}

<<tilde.epsilon.WW,eval=TRUE,echo=FALSE>>=
tilde.epsilon.WW<-qnorm((0.99+1)/2)*(1/NS)*sqrt((var1/proba1)+(var0/(1-proba1)))
@
\noindent{Finally, I estimate the effect of sampling noise on WW with 99\% confidence is $\bm{\Sexpr{tilde.epsilon.WW}}$. }

\noindent{In order to show the effect of sampling noise better, I do the graph as following:}

<<monte.carlo,eval=TRUE,echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,results='hide',cache=TRUE>>=
monte.carlo.ww <- function(s,N,param){
  set.seed(s)
  mu <- rnorm(N,param["barmu"],sqrt(param["sigma2mu"]))
  UB <- rnorm(N,0,sqrt(param["sigma2U"]))
  yB <- mu + UB 
  YB <- exp(yB)
  Ds <- rep(0,N)
  V <- rnorm(N,param["barmu"],sqrt(param["sigma2mu"]+param["sigma2U"]))
  Ds[V<=log(param["barY"])] <- 1 
  epsilon <- rnorm(N,0,sqrt(param["sigma2epsilon"]))
  eta<- rnorm(N,0,sqrt(param["sigma2eta"]))
  U0 <- param["rho"]*UB + epsilon
  y0 <- mu +  U0 + param["delta"]
  alpha <- param["baralpha"]+  param["theta"]*mu + eta
  y1 <- y0+alpha
  Y0 <- exp(y0)
  Y1 <- exp(y1)
  y <- y1*Ds+y0*(1-Ds)
  Y <- Y1*Ds+Y0*(1-Ds)
  return(c((1/sum(Ds))*sum(y*Ds)-(1/sum(1-Ds))*sum(y*(1-Ds)),var(y1[Ds==1]),var(y0[Ds==0]),mean(Ds)))
}


simuls.ww.N <- function(N,Nsim,param){
  simuls.ww <- matrix(unlist(lapply(1:Nsim,monte.carlo.ww,N=N,param=param)),
                      nrow=Nsim,ncol=4,byrow=TRUE)
  colnames(simuls.ww) <- c('WW','V1','V0','p')
  return(simuls.ww)
}


sf.simuls.ww.N <- function(N,Nsim,param){
  sfInit(parallel=TRUE,cpus=8)
  sim <- matrix(unlist(sfLapply(1:Nsim,monte.carlo.ww,N=N,param=param)),nrow=Nsim,ncol=4,byrow=TRUE)
  sfStop()
  colnames(sim) <- c('WW','V1','V0','p')
  return(sim)
}

Nsim <- 1000
N.sample <- c(100,1000,10000,100000)

simuls.ww <- lapply(N.sample,sf.simuls.ww.N,Nsim=Nsim,param=param)
names(simuls.ww) <- N.sample
@

<<delta.y.ate,eval=TRUE,echo=FALSE,results='hide'>>=
delta.y.ate <- function(paramS){
  return(paramS["baralpha"]+paramS["theta"]*paramS["barmu"])
}
@

<<precision,dependson='monte.carlo',eval=TRUE,echo=FALSE,results='hide',echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,fig.cap='Effect of sampling noise on the precision of the WW estimator (99\\% confidence)',fig.align='center',out.width='.65\\textwidth',fig.pos='htbp'>>=
delta <- 0.99
precision.ww <- function(k){
  return(2*quantile(abs(simuls.ww[[k]][,'WW']-delta.y.ate(paramS)),probs=c(delta)))
}
precision.ww.N <- sapply(1:length(simuls.ww),precision.ww)
precision <- as.data.frame(cbind(N.sample,precision.ww.N,rep(delta.y.ate(paramS),length(simuls.ww))))
colnames(precision) <- c('N','precision','TT')
@

<<CLTprec,eval=TRUE,echo=FALSE,results='hide'>>=
CLT.prec <- function(N,delta,v1,v0,p){
  return(2*qnorm((delta+1)/2)*sqrt((v1/p+v0/(1-p))/N))
}

CLT.prec.1 <- function(k,delta,simuls.ww){
  return(CLT.prec(as.numeric(names(simuls.ww)[[k]]),delta,simuls.ww[[k]][1,'V1'],simuls.ww[[k]][1,'V0'],simuls.ww[[k]][1,'p']))
}

CLT.prec.line <- function(k,delta,simuls.ww,N){
  return(CLT.prec(as.numeric(N),delta,simuls.ww[[as.character(N)]][k,'V1'],simuls.ww[[as.character(N)]][k,'V0'],simuls.ww[[as.character(N)]][k,'p']))
}

CLT.prec.mean <- function(k,delta,simuls.ww){
  return(mean(sapply(1:nrow(simuls.ww[[k]]),CLT.prec.line,delta=delta,simuls.ww=simuls.ww,N=as.numeric(names(simuls.ww)[[k]]))))
}

# I compute the precision for the first sample of each draw and the average sample size over draws
precision$precision.CLT.1 <- sapply(1:length(simuls.ww),CLT.prec.1,delta=delta,simuls.ww=simuls.ww)
precision$precision.CLT.mean <- sapply(1:length(simuls.ww),CLT.prec.mean,delta=delta,simuls.ww=simuls.ww)
@

<<precision.CLT,dependson='monte.carlo',eval=TRUE,echo=FALSE,results='hide',echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,fig.cap='sampling noise of the WW estimator with 99\\% confidence',fig.align='center',out.width='.65\\textwidth',fig.pos='htbp'>>=

ggplot(precision, aes(x=as.factor(N), y=TT)) +
  geom_bar(position=position_dodge(), stat="identity", colour='black') +
  geom_errorbar(aes(ymin=TT-precision/2, ymax=TT+precision/2), width=.2,position=position_dodge(.9),color='red') +
  geom_errorbar(aes(ymin=TT-precision.CLT.1/2, ymax=TT+precision.CLT.1/2), width=.2,position=position_dodge(.9),color='blue') +
  geom_errorbar(aes(ymin=TT-precision.CLT.mean/2, ymax=TT+precision.CLT.mean/2), width=.2,position=position_dodge(.9),color='green') +
  xlab("Sample Size") 
@

\noindent{Hmmm, this graph doesn't look that nice, but in fact the graph below is what I really get in R, I don't understand why it's not the same as what it's shown in R.}
\clearpage

\begin{figure}
\centering
\includegraphics[scale=0.45]{Rplot}
\caption{sampling noise of the WW estimator with 99\% confidence}
\end{figure}

\noindent{Well, this time the result is similar to yours.}

      \subsection{Estimate sampling noise using the same formula but for a confidence of 95\%. Has sampling noise increased or decreased?}
      
<<tilde.epsilon.WW.95,eval=TRUE,echo=FALSE>>=
tilde.epsilon.WW.95<-qnorm((0.95+1)/2)*(1/NS)*sqrt((var1/proba1)+(var0/(1-proba1)))
@
\noindent{With the formula that I showed in the last question, I estimate the effect of sampling noise on WW with 95\% confidence is $\bm{\Sexpr{tilde.epsilon.WW.95}}$, which means the sampling noise has decreased.}

      \subsection{Use the CLT formula to compute the sample size required to reach the amount of sampling noise that you have derived in question 9 with 99\% confidence. For this, use the variance of $\text{y^B}$ for the treated and untreated as an estimate of the two variances. Do you find a results similar to mine?}

According to the course, we learnt the CLT formula to compute the sample size required to reach the amount of sampling noise: \\
\centerline{$\displaystyle\tilde{N} & = \displaystyle\frac{1}{\epsilon^2}\left(\Phi^{-1}\left(\displaystyle\frac{\delta+1}{2}\right)\right)^2\left(\displaystyle\frac{\var{Y_i^1|D_i=1}}{\Pr(D_i=1)}+\displaystyle\frac{\var{Y_i^0|D_i=0}}{1-\Pr(D_i=1)}\right)$} \\

\noindent{In \textbf{1.2.8}, I have already gotten $\Pr(D_i=1)=\Sexpr{proba1}$ and $\tilde{\epsilon}=\Sexpr{tilde.epsilon.WW}$.}
\noindent{Since the question ask us to use the variance of $\text{y^B}$ for the treated and untreated as an estimate of the two variances, but in this FPSI context, you let us generate data with selection rule $D_i = \uns{V_i\leq\bar{y}}$, instead of $D_i = \uns{y_i^B\leq\bar{y}}$, so the $\text{y^B}$ here should be $\text{V}$, then I will firstly compute the variance of $\text{V}$ for the treated and untreated.}

<<varV,eval=TRUE,echo=FALSE>>=
varV1<-var(V[Ds==1])
varV0<-var(V[Ds==0])
@

\noindent{According to the calculating in R, I get $\bm{\var{V|D_i=1}=\Sexpr{varV1}}$ and $\bm{\var{V|D_i=0}=\Sexpr{varV0}}$.}

<<tilde.N,eval=TRUE,echo=FALSE>>=
tilde.N<-(1/(tilde.epsilon.WW^2))*(qnorm((0.99+1)/2))^2*((varV1/proba1)+(varV0/(1-proba1)))
@

\noindent{According to the formula above, I get $\bm{\tilde{N}=\Sexpr{tilde.N}}$, so the sample size should be $\bm{\Sexpr{tilde.N}}.}

\noindent{Moreover, I would like to do the following graph like what you did in the course.}

<<CLTsamp,eval=TRUE,echo=FALSE,results='hide'>>=
CLT.samp <- function(epsilon,delta,v1,v0,p){
  return((1/(epsilon^2))*(qnorm((delta+1)/2))^2*(v1/p+v0/(1-p)))
}

CLT.samp.1 <- function(N,delta,simuls.ww){
  return(CLT.samp(precision$precision[which(precision$N==N)]/2,delta,simuls.ww[[as.character(N)]][1,'V1'],simuls.ww[[as.character(N)]][1,'V0'],simuls.ww[[as.character(N)]][1,'p']))
}

CLT.samp.line <- function(k,delta,simuls.ww,N){
  return(CLT.samp(precision$precision[which(precision$N==N)]/2,delta,simuls.ww[[as.character(N)]][k,'V1'],simuls.ww[[as.character(N)]][k,'V0'],simuls.ww[[as.character(N)]][k,'p']))
}

CLT.samp.mean <- function(k,delta,simuls.ww){
  return(mean(sapply(1:nrow(simuls.ww[[k]]),CLT.samp.line,delta=delta,simuls.ww=simuls.ww,N=as.numeric(names(simuls.ww)[[k]]))))
}

# I compute the precision for the first sample of each draw and the average precision over draws
precision$samp.CLT.1 <- sapply(as.numeric(names(simuls.ww)),CLT.samp.1,delta=delta,simuls.ww=simuls.ww)
precision$samp.CLT.mean <- sapply(1:length(simuls.ww),CLT.samp.mean,delta=delta,simuls.ww=simuls.ww)
@

<<sample.CLT,dependson='monte.carlo',eval=TRUE,echo=FALSE,results='hide',echo=FALSE,warning=FALSE,error=FALSE,message=FALSE,fig.cap='Sample size using CLT',fig.align='center',out.width='.65\\textwidth',fig.pos='htbp'>>=
samp.t <- cbind(precision[,c('N','precision')],rep('Truth',length(precision$N)))
colnames(samp.t) <- c('N','precision','Method')
samp.1 <- cbind(precision[,c('samp.CLT.1','precision')],rep('CLT.1',length(precision$N)))
colnames(samp.1) <- c('N','precision','Method')
samp.mean <- cbind(precision[,c('samp.CLT.mean','precision')],rep('CLT.mean',length(precision$N)))
colnames(samp.mean) <- c('N','precision','Method')
sample.size.CLT <- as.data.frame(rbind(samp.t,samp.1,samp.mean))
sample.size.CLT$precision <- as.character(round(sample.size.CLT$precision,digits=2))
sample.size.CLT$precision <- factor(sample.size.CLT$precision,levels=sort(levels(as.factor(sample.size.CLT$precision)),decreasing=T))

ggplot(sample.size.CLT, aes(x=precision, y=N,fill=Method)) +
  geom_bar(position=position_dodge(), stat="identity", colour='black') #+
#  scale_y_continuous(trans='log10')
@

\noindent{Ops... the same problem as 1.2.8... In fact the graph that generated by R is as following:}
\clearpage

\begin{figure}
\centering
\includegraphics[scale=0.5]{CLT}
\caption{Sample size using CLT}
\end{figure}

\noindent{Well, this time the result is similar to yours.}

      \subsection{What happens to sample size if you use the same variance (that of $\text{y^B}$ overall)?}
Aa what I said in the last question, I use $\var{V}$ instead of $\var{y^B}$.

<<tilde.N.bis,eval=TRUE,echo=FALSE>>=
N.bis<-(1/(tilde.epsilon.WW^2))*(qnorm((0.99+1)/2))^2*((var(V)/proba1)+(var(V)/(1-proba1)))
@

\noindent{Thus, if I use the same variance $\var{V}$, then the sample size is $\bm{\Sexpr{N.bis}}$, which means the sample size increases.}

      \subsection{Estimate sample size with a smaller sampling noise but the same level of confidence. Does sampe size increase or decrease?}

The sampling noise in 1.2.10 is $\Sexpr{tilde.epsilon.WW}$, now I suppose a smaller sampling noise $\bm{0.002087}$. Then I will estimate sample size with this new smaller sampling noise and the same level of confidence 99\%.}

<<tilde.N.new,eval=TRUE,echo=FALSE>>=
N.new<-(1/(0.002087^2))*(qnorm((0.99+1)/2))^2*((varV1/proba1)+(varV0/(1-proba1)))
@

\noindent{The new sample size is $\bm{\Sexpr{N.new}}$, which is bigger than the sample size in 1.2.10, so the sample size increases.}

      \subsection{Estimate sample size with 95\% confidence for the same values of sampling noise as in questions 10 and 12. Does sample size increase or decrease?}
The sampling noise in question 10 is $\Sexpr{tilde.epsilon.WW}$, the one in question 12 is 0.002087.

<<N.10,eval=TRUE,echo=FALSE>>=
N.10<-(1/(tilde.epsilon.WW^2))*(qnorm((0.95+1)/2))^2*((varV1/proba1)+(varV0/(1-proba1)))
@

<<N.12,eval=TRUE,echo=FALSE>>=
N.12<-(1/(0.002087^2))*(qnorm((0.95+1)/2))^2*((varV1/proba1)+(varV0/(1-proba1)))
@

\vspace{.5em}
\noindent{Using R, I estimate the sample size with the sampling noise in question 10 is $\bm{\Sexpr{N.10}}$, the one with the sampling noise in question 12 is $\bm{\Sexpr{N.12}}$.}

\vspace{.5em}
\noindent{Now, I will compare the sample size in different situation.}

\vspace{.5em}
\noindent{With 99\% confidence, when the sampling noise is $\Sexpr{tilde.epsilon.WW}$, the sample size is $\Sexpr{tilde.N}$; when the sampling noise is 0.002087, the sample size is $\Sexpr{N.new}$.} 

\vspace{.5em}
\noindent{With 95\% confidence, when the sampling noise is $\Sexpr{tilde.epsilon.WW}$, the sample size is $\Sexpr{N.10}$; when the sampling noise is 0.002087, the sample size is $\Sexpr{N.12}$.}

\vspace{.5em}
\noindent{Comparing the results above, we can easily observe that for the same confidence level, sample size decreases with sampling noise; for the same sampling noise, sample size increases  with confidence level.}


\end{document}
